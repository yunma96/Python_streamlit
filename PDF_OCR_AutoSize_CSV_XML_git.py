# 25.08.21 11:45 ë°œë¦¬ìŠ¤ íŒ¨í„´ ì„ íƒ ì‹œ CSV ì €ì¥ í˜•ì‹ ë³€ê²½ ê¸°ëŠ¥ ì¶”ê°€
# 25.08.26 15:45 git ì„œë²„ì— ì˜¬ë ¤ streamlit cloudì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ Tesseract-OCR í´ë” ì§€ì •í•˜ë©´ ì•ˆë¨

import streamlit as st
import fitz  # PyMuPDF
import pytesseract
from PIL import Image, ImageEnhance, ImageDraw
import io
import os
import re
import pandas as pd
import traceback
import unicodedata
from collections import defaultdict
import xml.etree.ElementTree as ET
from xml.dom import minidom
import uuid
import csv

# --- 1. Tesseract ì‹¤í–‰ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì§€ì • ---
import sys
import os

# EXEë¡œ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ê²½ë¡œë¥¼ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜
def resource_path(relative_path):
    """ Get absolute path to resource, works for dev and for PyInstaller """
    try:
        # PyInstaller creates a temp folder and stores path in _MEIPASS
        base_path = sys._MEIPASS
    except Exception:
        base_path = os.path.abspath(".")

    return os.path.join(base_path, relative_path)

# --- 1. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ (OCR ë¶„ì„) ---

def run_ocr_on_page(page, top_margin, bottom_margin, dpi, tile_width, tile_height, overlap, rotation_angle, enhance_image):
    """
    ë‹¨ì¼ í˜ì´ì§€ì— ëŒ€í•´ OCRì„ ìˆ˜í–‰í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” í•µì‹¬ ë¡œì§ (UI ì¶œë ¥ ì—†ìŒ)
    """
    page_text = ""
    original_rect = page.rect
    processing_rect = fitz.Rect(
        original_rect.x0,
        original_rect.y0 + original_rect.height * top_margin,
        original_rect.x1,
        original_rect.y1 - original_rect.height * bottom_margin
    )
    
    num_tiles_x = int(processing_rect.width // tile_width) + 1
    num_tiles_y = int(processing_rect.height // tile_height) + 1

    for y in range(num_tiles_y):
        for x in range(num_tiles_x):
            x0 = processing_rect.x0 + x * tile_width
            y0 = processing_rect.y0 + y * tile_height
            x1 = min(x0 + tile_width, processing_rect.x1)
            y1 = min(y0 + tile_height, processing_rect.y1)
            
            x0_overlap = max(processing_rect.x0, x0 - overlap)
            y0_overlap = max(processing_rect.y0, y0 - overlap)
            x1_overlap = min(processing_rect.x1, x1 + overlap)
            y1_overlap = min(processing_rect.y1, y1 + overlap)
            
            clip_rect = fitz.Rect(x0_overlap, y0_overlap, x1_overlap, y1_overlap)

            pix = page.get_pixmap(clip=clip_rect, dpi=dpi)
            if pix.width == 0 or pix.height == 0:
                continue

            img_bytes = pix.tobytes("png")
            image = Image.open(io.BytesIO(img_bytes))

            if rotation_angle != 0:
                image = image.rotate(-rotation_angle, expand=True)

            if enhance_image:
                enhancer = ImageEnhance.Contrast(image)
                image = enhancer.enhance(2.0)
                enhancer = ImageEnhance.Sharpness(image)
                image = enhancer.enhance(2.0)

            image = image.convert('L')
            image = image.point(lambda p: 0 if p < 180 else 255, '1')

            try:
                ocr_text = pytesseract.image_to_string(image, lang='eng', config='--psm 3')
                page_text += ocr_text + "\n"
            except Exception:
                continue
    return page_text


def extract_text_from_pdf_ocr(uploaded_file, top_margin, bottom_margin, dpi, tile_width, tile_height, overlap, rotation_angle, enhance_image):
    """
    ì—…ë¡œë“œëœ PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³ , íƒ€ì¼ ë¶„í•  ì‹œê°í™” ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (UI ì¶œë ¥ í¬í•¨)
    """
    Image.MAX_IMAGE_PIXELS = None
    full_text = ""
    visualization_images = []
    
    file_bytes = uploaded_file.getvalue()
    
    try:
        doc = fitz.open(stream=file_bytes, filetype="pdf")
        st.info(f"ì´ {len(doc)} í˜ì´ì§€ì˜ PDF íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
        progress_bar = st.progress(0)
        
        for i, page in enumerate(doc):
            page_num = i + 1
            st.write(f"í˜ì´ì§€ {page_num}/{len(doc)} ì²˜ë¦¬ ì¤‘...")

            vis_dpi = 150
            page_pix = page.get_pixmap(dpi=vis_dpi)
            page_image = Image.open(io.BytesIO(page_pix.tobytes("png"))).convert("RGBA")
            draw = ImageDraw.Draw(page_image)
            scale = vis_dpi / 72.0

            original_rect = page.rect
            processing_rect = fitz.Rect(
                original_rect.x0,
                original_rect.y0 + original_rect.height * top_margin,
                original_rect.x1,
                original_rect.y1 - original_rect.height * bottom_margin
            )
            
            proc_rect_coords = [
                (processing_rect.x0 - original_rect.x0) * scale,
                (processing_rect.y0 - original_rect.y0) * scale,
                (processing_rect.x1 - original_rect.x0) * scale,
                (processing_rect.y1 - original_rect.y0) * scale
            ]
            draw.rectangle(proc_rect_coords, outline="blue", width=3)
            
            page_text = run_ocr_on_page(page, top_margin, bottom_margin, dpi, tile_width, tile_height, overlap, rotation_angle, enhance_image)
            full_text += page_text
            
            num_tiles_x = int(processing_rect.width // tile_width) + 1
            num_tiles_y = int(processing_rect.height // tile_height) + 1
            for y in range(num_tiles_y):
                for x in range(num_tiles_x):
                    x0 = processing_rect.x0 + x * tile_width
                    y0 = processing_rect.y0 + y * tile_height
                    x0_overlap = max(processing_rect.x0, x0 - overlap)
                    y0_overlap = max(processing_rect.y0, y0 - overlap)
                    x1_overlap = min(processing_rect.x1, x0 + tile_width + overlap)
                    y1_overlap = min(processing_rect.y1, y0 + tile_height + overlap)
                    clip_rect = fitz.Rect(x0_overlap, y0_overlap, x1_overlap, y1_overlap)
                    tile_rect_coords = [
                        (clip_rect.x0 - original_rect.x0) * scale,
                        (clip_rect.y0 - original_rect.y0) * scale,
                        (clip_rect.x1 - original_rect.x0) * scale,
                        (clip_rect.y1 - original_rect.y0) * scale
                    ]
                    draw.rectangle(tile_rect_coords, outline="red", width=2)

            visualization_images.append(page_image)
            progress_bar.progress((i + 1) / len(doc))
        
        return full_text, visualization_images

    except Exception as e:
        st.error(f"PDF ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.error(traceback.format_exc())
        return None, []

def parse_all_potential_data(raw_text):
    """
    OCR í…ìŠ¤íŠ¸ì—ì„œ ìœ„ì¹˜ì™€ IDë¥¼ ì°¾ì•„ ë°ì´í„° ìŒì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    location_pattern = r"[Kk]\s*[Mm]\s*[0-9OQ_]+\s*\+\s*[0-9OQ_]+"
    id_extraction_pattern = r"[A-Za-z0-9/_-]+"
    
    text_blocks = re.split(f'({location_pattern})', raw_text)
    
    structured_data = []
    
    for i in range(1, len(text_blocks), 2):
        location_text = text_blocks[i]
        id_block = text_blocks[i+1] if i + 1 < len(text_blocks) else ""

        processed_location = location_text.replace('O', '0').replace('Q', '0')
        clean_location = "".join(processed_location.split()).replace('_', '').upper()
        
        lines = [line.strip() for line in id_block.split('\n') if line.strip()]
        if not lines:
            continue
        
        first_line = lines[0]

        if re.search(r'\b(0r|Dr|And|Etc)\b', first_line, re.IGNORECASE):
            continue
        
        first_line = re.sub(r'\bN\b', '', first_line, flags=re.IGNORECASE)
        first_line = re.sub(r'\s+', ' ', first_line).strip()
        if not first_line:
            continue
        
        corrected_line = re.sub(r'\s*[=,ã…¡â€”,â€”ã…¡,â€”,ã…¡]\s*', '-', first_line)
        corrected_line = re.sub(r'--+', '-', corrected_line)
        
        corrected_line = re.sub(r'\b(FP|P)\s+(\d+)\b', r'\1-\2', corrected_line, flags=re.IGNORECASE)
        
        corrected_line = re.sub(r'(\b\d{4})\s*(iN|in|i|N)\b', r'\1', corrected_line, flags=re.IGNORECASE)

        balise_pattern = r'([A-Za-z0-9]{3})\s*[-/_\s]\s*(R|SR|ST|SH|P1|P2)\s*[-/_\s]\s*([\w/-]+)'
        balise_matches = re.findall(balise_pattern, corrected_line, re.IGNORECASE)
        
        for match in balise_matches:
            prefix = match[0].upper().replace('0', 'O')
            middle = match[1].upper()
            suffix_part = match[2].upper()

            if '/' in suffix_part:
                base_id = f"{prefix}-{middle}-"
                split_suffixes = suffix_part.split('/')
                for part in split_suffixes:
                    if part:
                        marker_id = f"{base_id}{part}"
                        structured_data.append({
                            'location': clean_location,
                            'marker_id': marker_id
                        })
            else:
                marker_id = f"{prefix}-{middle}-{suffix_part}"
                structured_data.append({
                    'location': clean_location,
                    'marker_id': marker_id
                })
        
        corrected_line = re.sub(balise_pattern, '', corrected_line, flags=re.IGNORECASE)

        parts = re.findall(id_extraction_pattern, corrected_line)
        all_id_parts = [p for p in parts if p.strip()]

        j = 0
        while j < len(all_id_parts):
            current_part = all_id_parts[j]
            
            if j + 1 < len(all_id_parts):
                next_part = all_id_parts[j+1]
                if re.fullmatch(r'[A-Z]+', current_part, re.IGNORECASE) and not re.fullmatch(r'\d+', next_part):
                    potential_id = f"{current_part}-{next_part}"
                    j += 2
                elif re.fullmatch(r'[A-Z]{1,3}', current_part, re.IGNORECASE) and re.fullmatch(r'\d+', next_part):
                    potential_id = f"{current_part} {next_part}"
                    j += 2
                elif re.search(r'/\d*$', current_part) and re.fullmatch(r'\d+', next_part):
                    potential_id = f"{current_part}{next_part}"
                    j += 2
                elif re.search(r'-\d$', current_part) and re.match(r'\d+/', next_part):
                    potential_id = f"{current_part}{next_part}"
                    j += 2
                else:
                    potential_id = current_part
                    j += 1
            else:
                potential_id = current_part
                j += 1
            
            corrected_id = potential_id

            if re.fullmatch(r"[-_/\\]+", corrected_id) or len(corrected_id) < 2:
                continue
            
            if len(corrected_id) >= 3 and len(set(corrected_id)) == 1 and not corrected_id.isalnum():
                continue

            if re.search(r'[^\w\s/-]', corrected_id):
                continue

            if corrected_id.startswith('-'):
                corrected_id = corrected_id[1:]
            
            corrected_id = re.sub(r'-[A-Za-z]{1,2}-?$', '', corrected_id, flags=re.IGNORECASE)

            if corrected_id:
                structured_data.append({
                    'location': clean_location,
                    'marker_id': corrected_id.upper()
                })

    final_data = []
    seen = set()
    for item in structured_data:
        item_tuple = (item['location'], item['marker_id'])
        if item_tuple not in seen:
            final_data.append(item)
            seen.add(item_tuple)

    return final_data

def filter_displayed_data(full_df, selected_patterns, min_id_length, show_all, require_digit):
    """
    ì‚¬ì´ë“œë°”ì˜ í•„í„° ì¡°ê±´ì— ë”°ë¼ í‘œì‹œí•  ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if full_df is None or full_df.empty or 'marker_id' not in full_df.columns:
        return pd.DataFrame(columns=full_df.columns)

    filtered_df = full_df.copy()

    if not show_all:
        if require_digit:
            digit_mask = filtered_df['marker_id'].str.contains(r'\d', regex=True, na=False)
            filtered_df = filtered_df[digit_mask]

        if selected_patterns:
            try:
                id_pattern = "|".join([f"({p})" for p in selected_patterns])
                anchored_pattern = f"^({id_pattern})$"
                pattern_mask = filtered_df['marker_id'].str.match(anchored_pattern, na=False, case=False)
                filtered_df = filtered_df[pattern_mask]
            except re.error as e:
                st.error(f"ì˜¤ë¥˜: íŒ¨í„´ì´ ìœ íš¨í•œ ì •ê·œí‘œí˜„ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. ì—ëŸ¬: {e}")
                return pd.DataFrame(columns=full_df.columns)
        else:
            pass
        
        length_mask = filtered_df['marker_id'].str.replace(r'\s+', '', regex=True).str.len() >= min_id_length
        filtered_df = filtered_df[length_mask]

    if not show_all and not selected_patterns:
        st.warning("ê²°ê³¼ë¥¼ ë³´ë ¤ë©´ 'ëª¨ë“  ì ì¬ì  ID í‘œì‹œ'ë¥¼ ì„ íƒí•˜ê±°ë‚˜, í•˜ë‚˜ ì´ìƒì˜ íŒ¨í„´ í•„í„°ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
        return pd.DataFrame(columns=full_df.columns)
    
    return filtered_df

def safe_filename(filename):
    """
    íŒŒì¼ ì´ë¦„ì—ì„œ ìœ ë‹ˆì½”ë“œ ë¬¸ìë¥¼ ASCII ë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    decomposed = unicodedata.normalize('NFD', filename)
    ascii_filename = decomposed.encode('ascii', 'ignore').decode('utf-8')
    safe_name = re.sub(r'[^a-zA-Z0-9._-]', '_', ascii_filename)
    return safe_name

# --- 3. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ (XML ìƒì„±) ---

def parse_coord_string_xy(coord_str):
    """'{X=123.4, Y=567.8}' í˜•ì‹ì˜ ë¬¸ìì—´ì—ì„œ X, Y ì¢Œí‘œë¥¼ floatìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if not coord_str: return None, None
    match_x = re.search(r'X=\s*(-?[\d\.]+)', coord_str)
    match_y = re.search(r'Y=\s*(-?[\d\.]+)', coord_str)
    x = float(match_x.group(1)) if match_x else None
    y = float(match_y.group(1)) if match_y else None
    return x, y

def extract_data_from_xml_root(root):
    """
    íŒŒì‹±ëœ XML ë£¨íŠ¸ì—ì„œ RailLine ë°ì´í„°ì™€ StartRealKPValueë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    try:
        start_kp_value = 0.0
        start_kp_prop = root.find(".//RailRealKPList/RealKP/property[@name='StartRealKPValue']")
        if start_kp_prop is not None and start_kp_prop.get('value'):
            try:
                start_kp_value = float(start_kp_prop.get('value'))
                st.success(f"ì„±ê³µ: StartRealKPValue '{start_kp_value}' ê°’ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            except (ValueError, TypeError):
                st.warning(f"ê²½ê³ : StartRealKPValue ê°’ì„ ìˆ«ìë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ 0ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            st.warning("ê²½ê³ : StartRealKPValueë¥¼ XMLì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ 0ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

        raillines_data = []
        for rail_line in root.findall('.//RailLines/RailLine'):
            rail_line_id = rail_line.get('Id')
            all_x_coords = []
            all_y_coords = []
            for line_segment in rail_line.findall('.//Lines/Line'):
                p1_prop = line_segment.find("property[@name='P1']")
                p2_prop = line_segment.find("property[@name='P2']")
                if p1_prop is not None and p2_prop is not None:
                    p1_x, p1_y = parse_coord_string_xy(p1_prop.get('value'))
                    p2_x, p2_y = parse_coord_string_xy(p2_prop.get('value'))
                    if p1_x is not None: all_x_coords.append(p1_x)
                    if p2_x is not None: all_x_coords.append(p2_x)
                    if p1_y is not None: all_y_coords.append(p1_y)
                    if p2_y is not None: all_y_coords.append(p2_y)
            if all_x_coords:
                start_x, end_x = min(all_x_coords), max(all_x_coords)
                furthest_y = max(all_y_coords, key=abs) if all_y_coords else 0
                raillines_data.append({'Id': rail_line_id, 'StartX': start_x, 'EndX': end_x, 'FurthestY': furthest_y})

        st.success(f"ì„±ê³µ: {len(raillines_data)}ê°œì˜ RailLine ì „ì²´ êµ¬ê°„ ì •ë³´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return raillines_data, start_kp_value
    except Exception as e:
        st.error(f"ì˜¤ë¥˜: XML ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, 0

def find_railline_for_kp(kp_x, kp_y, raillines_data):
    """ì£¼ì–´ì§„ KP_X, KP_Y ê°’ì— ê°€ì¥ ì í•©í•œ RailLineì„ ì°¾ìŠµë‹ˆë‹¤."""
    candidate_raillines = []
    for railline in raillines_data:
        if railline['StartX'] <= kp_x <= railline['EndX']:
            candidate_raillines.append(railline)
    
    if not candidate_raillines:
        return None
        
    best_match = min(candidate_raillines, key=lambda r: abs(r['FurthestY'] - kp_y))
    return best_match
    
def create_property(parent, name, value):
    """'property' ìš”ì†Œë¥¼ ìƒì„±í•˜ê³  ë¶€ëª¨ ìš”ì†Œì— ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜"""
    prop = ET.SubElement(parent, 'property')
    prop.set('name', name)
    prop.set('value', str(value))
    return prop

def generate_signal_data(signal_csv_file, raillines_data, start_real_kp_value, insertion_point):
    """
    ì‹ í˜¸ê¸° CSVë¥¼ ì½ì–´ ê¸°ì¡´ RailSignal ìš”ì†Œì— ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    SignalGroup IDê°€ ê¸°ì¡´ IDì˜ ìµœëŒ“ê°’ì— ì´ì–´ì„œ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±ë˜ë„ë¡ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.
    """
    try:
        csv_content = signal_csv_file.getvalue().decode('utf-8-sig')
        csv_reader = csv.DictReader(io.StringIO(csv_content))
        
        required_columns = ['KPPoint_X', 'KPPoint_Y', 'Direction', 'Code', 'DefaultCode', 'DisplayName', 'PoleType']
        # CSV íŒŒì¼ì— í•„ìˆ˜ ì—´ì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        for col in required_columns:
            if col not in csv_reader.fieldnames:
                st.error(f"ì˜¤ë¥˜: ì‹ í˜¸ê¸° CSV íŒŒì¼ì— í•„ìˆ˜ ì—´ì¸ '{col}'ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

        # XMLì—ì„œ RailSignal ìš”ì†Œë¥¼ ì°¾ê±°ë‚˜, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        rail_signal_root = insertion_point.find('RailSignal')
        if rail_signal_root is None:
            rail_signal_root = ET.SubElement(insertion_point, 'RailSignal', Id="RSG_1", Name="RSG_1")
            create_property(rail_signal_root, "Id", "RSG_1")
            create_property(rail_signal_root, "Name", "RSG_1")

        # RailSignal ìš”ì†Œ ì•„ë˜ì— SignalGroupListë¥¼ ì°¾ê±°ë‚˜, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        signal_group_list = rail_signal_root.find('SignalGroupList')
        if signal_group_list is None:
            signal_group_list = ET.SubElement(rail_signal_root, 'SignalGroupList')

        # --- ì½”ë“œ ìˆ˜ì • ë¶€ë¶„ ì‹œì‘ ---
        # ê¸°ì¡´ SignalGroup IDì˜ ìµœëŒ“ê°’ì„ ì°¾ìŠµë‹ˆë‹¤.
        max_id = -1
        existing_groups = signal_group_list.findall('SignalGroup')
        if existing_groups:
            for group in existing_groups:
                group_id_str = group.get('Id', 'SG_-1')
                try:
                    # 'SG_123'ê³¼ ê°™ì€ IDì—ì„œ ìˆ«ì ë¶€ë¶„ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
                    numeric_part = int(re.search(r'\d+', group_id_str).group())
                    if numeric_part > max_id:
                        max_id = numeric_part
                except (ValueError, AttributeError):
                    # ID í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¥¼ ê²½ìš° ê±´ë„ˆëœë‹ˆë‹¤.
                    st.warning(f"ê²½ê³ : SignalGroup ID '{group_id_str}'ì˜ ìˆ«ì ë¶€ë¶„ì„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue
        
        # ìƒˆ SignalGroup IDì˜ ì‹œì‘ ë²ˆí˜¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        start_index = max_id + 1
        # --- ì½”ë“œ ìˆ˜ì • ë¶€ë¶„ ë ---

        # CSV íŒŒì¼ì˜ ê° í–‰ì„ ìˆœíšŒí•˜ë©° SignalGroupì„ ìƒì„±í•©ë‹ˆë‹¤.
        for i, row in enumerate(csv_reader):
            current_index = start_index + i
            
            # KPPoint_Xì™€ KPPoint_Y ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
            final_kp_x = float(row['KPPoint_X']) - start_real_kp_value
            final_kp_y = float(row['KPPoint_Y']) * 600
            
            # ì¢Œí‘œì— ë§ëŠ” RailLineì„ ì°¾ìŠµë‹ˆë‹¤.
            matched_railline = find_railline_for_kp(final_kp_x, final_kp_y, raillines_data)
            rail_line_id, rail_length = ("N/A", "N/A")
            if matched_railline:
                rail_line_id = matched_railline['Id']
                rail_length = int(final_kp_x - matched_railline['StartX'])
            else:
                st.warning(f"ì‹ í˜¸ê¸° ê²½ê³  (í–‰ {i+2}): KP_X ê°’ {final_kp_x}ì— í•´ë‹¹í•˜ëŠ” ë¼ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            # ìƒˆë¡œìš´ SignalGroup ìš”ì†Œë¥¼ ìƒì„±í•˜ê³  IDë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
            sg = ET.SubElement(signal_group_list, 'SignalGroup', Id=f"SG_{current_index}")
            create_property(sg, "Id", f"SG_{current_index}")
            create_property(sg, "Name", f"SG_{current_index}")
            
            signals = ET.SubElement(sg, 'Signals')
            signal = ET.SubElement(signals, 'Signal')
            
            # CSV ë°ì´í„°ì™€ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•˜ì—¬ propertyë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
            create_property(signal, "Id", f"Signal_{current_index}")
            create_property(signal, "KPPoint", f"{{X={final_kp_x}, Y={final_kp_y}}}")
            create_property(signal, "RailLineID", rail_line_id)
            create_property(signal, "Direction", row['Direction'])
            create_property(signal, "Code", row['Code'])
            create_property(signal, "DefaultCode", row['DefaultCode'])
            create_property(signal, "DisplayName", row['DisplayName'])
            create_property(signal, "PoleType", row['PoleType'])
            create_property(signal, "RailLength", rail_length)
            
            # ê³ ì •ëœ ê¸°ë³¸ propertyë“¤ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
            for name, value in [("Offset", "-300"), ("Type", "0"), ("IsSubLine", "False"), 
                                ("IsAutoChange", "True"), ("IsVertical", "False"), 
                                ("Color", "Color [Gray]"), ("Width", "2"), ("RailOffset", "-300"), 
                                ("RailHeightOffset", "30"), ("IsVisualText", "True")]:
                create_property(signal, name, value)

        st.success("ì„±ê³µ: ì‹ í˜¸ê¸° CSV íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ XMLì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        st.error(f"ì˜¤ë¥˜: ì‹ í˜¸ê¸° ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.error(traceback.format_exc())


def generate_balis_data(balis_csv_file, raillines_data, start_real_kp_value, insertion_point):
    """ë°œë¦¬ìŠ¤ CSVë¥¼ ì½ì–´ ê¸°ì¡´ RailBalis ìš”ì†Œì— ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
    try:
        csv_content = balis_csv_file.getvalue().decode('utf-8-sig')
        csv_reader = csv.DictReader(io.StringIO(csv_content))
        
        required_columns = ['KPPoint_X', 'KPPoint_Y', 'Direction', 'Name']
        for col in required_columns:
            if col not in csv_reader.fieldnames:
                st.error(f"ì˜¤ë¥˜: ë°œë¦¬ìŠ¤ CSV íŒŒì¼ì— í•„ìˆ˜ ì—´ì¸ '{col}'ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

        rail_balis_root = insertion_point.find('RailBalis')
        if rail_balis_root is None:
            balis_id = str(uuid.uuid4())
            rail_balis_root = ET.SubElement(insertion_point, 'RailBalis', Id=balis_id, Name="")
            create_property(rail_balis_root, "Id", balis_id)
            create_property(rail_balis_root, "Name", "")
        
        balis_list = rail_balis_root.find('BalisList')
        if balis_list is None:
            balis_list = ET.SubElement(rail_balis_root, 'BalisList')
        
        start_index = len(balis_list.findall('Balis'))

        for i, row in enumerate(csv_reader):
            current_index = start_index + i
            final_kp_x = float(row['KPPoint_X']) - start_real_kp_value
            final_kp_y = float(row['KPPoint_Y'])

            matched_railline = find_railline_for_kp(final_kp_x, final_kp_y, raillines_data)
            rail_line_id, rail_length = ("N/A", "N/A")
            if matched_railline:
                rail_line_id = matched_railline['Id']
                rail_length = int(final_kp_x - matched_railline['StartX'])
            else:
                st.warning(f"ë°œë¦¬ìŠ¤ ê²½ê³  (í–‰ {i+2}): KP_X ê°’ {final_kp_x}ì— í•´ë‹¹í•˜ëŠ” ë¼ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            balis = ET.SubElement(balis_list, 'Balis')
            create_property(balis, "Id", str(current_index))
            create_property(balis, "KPPoint", f"{{X={final_kp_x}, Y={final_kp_y}}}")
            create_property(balis, "Name", row['Name'])
            create_property(balis, "Direction", row['Direction'])
            create_property(balis, "RailLineId", rail_line_id)
            create_property(balis, "RailLength", rail_length)
            for name, value in [("SignalCount", "0"), ("TypeCode", "0"),
                                ("strLinkSignalName", ""), ("BalisType", "2"), ("ERTMS_Mode", "0"),
                                ("ERTMS_Level", "0"), ("ERTMS_StartKP", "0"), ("ERTMS_EndKP", "0"),
                                ("RailOffset", "0"), ("RailHeightOffset", "-23"), ("BalisActive", "True")]:
                create_property(balis, name, value)
        st.success("ì„±ê³µ: ë°œë¦¬ìŠ¤ CSV íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        st.error(f"ì˜¤ë¥˜: ë°œë¦¬ìŠ¤ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# --- 4. Streamlit UI ë° ìƒíƒœ ê´€ë¦¬ ---
st.set_page_config(layout="wide")
st.title("ğŸ“„ PDF OCR ë° íŒ¨í„´ ë¶„ì„ê¸°")


# try:
#     # Tesseract ì‹¤í–‰ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì§€ì •
#     # ìœ„ì—ì„œ --add-data ì˜µì…˜ìœ¼ë¡œ "Tesseract-OCR" í´ë”ë¥¼ ì¶”ê°€í–ˆìœ¼ë¯€ë¡œ
#     # EXE ë‚´ë¶€ì—ì„œëŠ” í•´ë‹¹ í´ë” ì•ˆì˜ tesseract.exeë¥¼ ì°¾ì•„ì•¼ í•¨
#     tesseract_cmd_path = resource_path("Tesseract-OCR/tesseract.exe")
#     pytesseract.pytesseract.tesseract_cmd = tesseract_cmd_path
#     # pytesseract.pytesseract.tesseract_cmd = r"C:\Users\Administrator\AppData\Local\Programs\Tesseract-OCR\tesseract.exe"
# except Exception:
#     st.warning("Tesseract ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ ë‚´ì—ì„œ ê²½ë¡œë¥¼ ì§ì ‘ ì„¤ì •í•´ì£¼ì„¸ìš”.")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'full_results_df' not in st.session_state:
    st.session_state.full_results_df = None
if 'raw_text' not in st.session_state:
    st.session_state.raw_text = None
if 'visualization_images' not in st.session_state:
    st.session_state.visualization_images = []
if 'uploaded_file_name' not in st.session_state:
    st.session_state.uploaded_file_name = ""
if 'reliability_results_df' not in st.session_state:
    st.session_state.reliability_results_df = None
if 'edited_df' not in st.session_state:
    st.session_state.edited_df = None
if 'data_with_selection' not in st.session_state:
    st.session_state.data_with_selection = None
if 'generated_xml' not in st.session_state:
    st.session_state.generated_xml = None

# ì‚¬ì´ë“œë°” UI
with st.sidebar:
    st.header("âš™ï¸ ë¶„ì„ ì„¤ì •")
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=["pdf"])
    
    st.subheader("í˜ì´ì§€ íšŒì „")
    rotation_angle = st.selectbox("íšŒì „ ê°ë„", [0, 90, 180, 270], index=0)

    st.subheader("ì˜ì—­ ì„¤ì •")
    top_margin = st.slider("ìƒë‹¨ ì œì™¸ ë¹„ìœ¨(%)", 0, 100, 15) / 100.0
    bottom_margin = st.slider("í•˜ë‹¨ ì œì™¸ ë¹„ìœ¨(%)", 0, 100, 15) / 100.0
    
    st.subheader("OCR ì„¤ì •")
    dpi = st.slider("DPI (í•´ìƒë„)", 100, 800, 550, 50)
    
    st.subheader("íƒ€ì¼ ì„¤ì •")
    tile_width = st.slider("íƒ€ì¼ ë„ˆë¹„", 100, 1500, 1000, 50)
    tile_height = st.slider("íƒ€ì¼ ë†’ì´", 100, 1500, 1000, 50)
    overlap = st.slider("íƒ€ì¼ ê²¹ì¹¨ (pixels)", 0, 200, 20, 10)

    st.subheader("ë¶„ì„ ì‹¤í–‰")
    col1, col2 = st.columns(2)
    with col1:
        analyze_button = st.button("ë‹¨ì¼ ë¶„ì„", use_container_width=True)
    with col2:
        reliability_button = st.button("ì‹ ë¢°ë„ ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True)

    with st.expander("ì‹ ë¢°ë„ ë¶„ì„ íƒ€ì¼ ì„¤ì •"):
        tile_options_input = st.text_area(
            "í…ŒìŠ¤íŠ¸í•  íƒ€ì¼ í¬ê¸° ì…ë ¥ (ë„ˆë¹„,ë†’ì´)",
            value="100,100\n200,200\n300,300\n1000,1000",
            help="í•œ ì¤„ì— 'ë„ˆë¹„,ë†’ì´' í˜•ì‹ìœ¼ë¡œ í•˜ë‚˜ì”© ì…ë ¥í•˜ì„¸ìš”."
        )

    st.markdown("---")
    st.header("ğŸ“Š ê²°ê³¼ í•„í„°ë§")
    
    show_all = st.checkbox("ëª¨ë“  ì ì¬ì  ID í‘œì‹œ", value=False)
    require_digit = st.checkbox("IDì— ìˆ«ì í¬í•¨", value=True, disabled=show_all)
    min_id_length = st.slider("ìµœì†Œ ID ê¸¸ì´", 1, 10, 3, disabled=show_all)
    
    st.subheader("íŒ¨í„´ ì„ íƒ")
    
    predefined_patterns = {
        "ë˜‘ë˜‘í•œ ë²”ìš© íŒ¨í„´ (ì¶”ì²œ)": r"[A-Za-z0-9]+[-_][A-Za-z0-9_/-]+",
        "'WD' íŒ¨í„´": r"W\s*D\s*\d+",
        "'FP/P' íŒ¨í„´": r"(FP|P)\s*[-_/]?\s*\d+",
        "ì‹ í˜¸ê¸° íŒ¨í„´ (ìˆ«ì ë˜ëŠ” ìˆ«ì/ìˆ«ì)": r"\b\d{4}\b\s*[-/]\s*\b\d{4}\b|\b\d{4}\b|(EOL)[-_]\s*\b\d{4}[A-Za-z]?\b",
        "ë°œë¦¬ìŠ¤ íŒ¨í„´ (ë²”ìš©)": r"[A-Z]{3}-(R|SR|ST|SH|P1|P2)-[\w/-]+"
    }

    selected_patterns = [p for label, p in predefined_patterns.items() if st.checkbox(label, value=False, disabled=show_all)]

# --- ë©”ì¸ í™”ë©´ íƒ­ ---
main_tab1, main_tab2, main_tab3 = st.tabs(["ë¶„ì„ ê²°ê³¼", "CSV í¸ì§‘ê¸°", "XML ìƒì„±ê¸°"])

with main_tab1:
    # --- ì‹ ë¢°ë„ ë¶„ì„ ë¡œì§ ---
    if reliability_button:
        if uploaded_file:
            results_with_source = defaultdict(set)
            file_bytes = uploaded_file.getvalue()
            doc = fitz.open(stream=file_bytes, filetype="pdf")
            
            tile_options = []
            if tile_options_input:
                lines = tile_options_input.strip().split('\n')
                for line in lines:
                    try:
                        width, height = map(int, line.strip().split(','))
                        tile_options.append({'width': width, 'height': height})
                    except ValueError:
                        st.error(f"ì˜ëª»ëœ íƒ€ì¼ í¬ê¸° í˜•ì‹ì…ë‹ˆë‹¤: '{line}'.")
                        st.stop()
            if not tile_options:
                st.error("ì‹ ë¢°ë„ ë¶„ì„ì„ ìœ„í•œ íƒ€ì¼ í¬ê¸°ë¥¼ í•˜ë‚˜ ì´ìƒ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                st.stop()
            
            quality_options = [{'enhance': True}, {'enhance': False}]
            total_tests = len(tile_options) * len(quality_options)

            with st.spinner(f"í†µí•© ì‹ ë¢°ë„ ë¶„ì„ ì¤‘... (ì´ í…ŒìŠ¤íŠ¸ ê°œìˆ˜: {total_tests})"):
                test_progress = st.progress(0)
                count = 0
                for tile_opt in tile_options:
                    for quality_opt in quality_options:
                        count += 1
                        current_tile_width = tile_opt['width']
                        current_tile_height = tile_opt['height']
                        current_enhance = quality_opt['enhance']
                        
                        quality_str = "í–¥ìƒ" if current_enhance else "ì›ë³¸"
                        st.write(f"í…ŒìŠ¤íŠ¸ {count}/{total_tests}: {current_tile_width}x{current_tile_height} ({quality_str})...")
                        source_str = f"{current_tile_width}x{current_tile_height} ({quality_str})"

                        full_text = ""
                        for page in doc:
                            full_text += run_ocr_on_page(page, top_margin, bottom_margin, dpi, current_tile_width, current_tile_height, overlap, rotation_angle, current_enhance)
                        
                        parsed_data = parse_all_potential_data(full_text)
                        for item in parsed_data:
                            key = (item['location'], item['marker_id'])
                            results_with_source[key].add(source_str)
                        test_progress.progress(count / total_tests)

            st.success("ì‹ ë¢°ë„ ë¶„ì„ ì™„ë£Œ!")
            
            reliability_data = []
            for (location, marker_id), sources in results_with_source.items():
                data_entry = {
                    "location": location,
                    "marker_id": marker_id,
                    "ë°œê²¬ íšŸìˆ˜": len(sources),
                    "ë°œê²¬ ì¡°ê±´": ", ".join(sorted(list(sources)))
                }
                reliability_data.append(data_entry)
                
            if reliability_data:
                df = pd.DataFrame(reliability_data)
                st.session_state.reliability_results_df = df.sort_values(by="ë°œê²¬ íšŸìˆ˜", ascending=False)
            else:
                st.session_state.reliability_results_df = pd.DataFrame()

            st.session_state.full_results_df = None
            st.session_state.uploaded_file_name = uploaded_file.name

        else:
            st.error("ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

    # ë‹¨ì¼ ë¶„ì„ ì‹œì‘ ë¡œì§
    if analyze_button and uploaded_file:
        with st.spinner("PDFë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”..."):
            enhance_image_single = True 
            raw_text, viz_images = extract_text_from_pdf_ocr(uploaded_file, top_margin, bottom_margin, dpi, tile_width, tile_height, overlap, rotation_angle, enhance_image_single)
            
            st.session_state.raw_text = raw_text
            st.session_state.visualization_images = viz_images
            st.session_state.uploaded_file_name = uploaded_file.name
            st.session_state.reliability_results_df = None

            if raw_text:
                parsed_data = parse_all_potential_data(raw_text)
                st.session_state.full_results_df = pd.DataFrame(parsed_data)
                st.success("ë¶„ì„ ì™„ë£Œ! ì•„ë˜ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³  í•„í„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            else:
                st.session_state.full_results_df = None
                st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # --- ê²°ê³¼ í‘œì‹œ ë° ë‹¤ìš´ë¡œë“œ ë¡œì§ (ê³µí†µ) ---
    df_to_display = None
    analysis_type = ""
    if st.session_state.reliability_results_df is not None:
        df_to_display = st.session_state.reliability_results_df
        analysis_type = "ì‹ ë¢°ë„"
        st.header("ğŸ”¬ í†µí•© ì‹ ë¢°ë„ ë¶„ì„ ê²°ê³¼")
    elif st.session_state.full_results_df is not None:
        df_to_display = st.session_state.full_results_df
        analysis_type = "ë‹¨ì¼"
        st.header("ğŸ“Š ë‹¨ì¼ ë¶„ì„ ê²°ê³¼")

    if df_to_display is not None:
        if not df_to_display.empty:
            
            if analysis_type == "ì‹ ë¢°ë„":
                max_count = 1
                if tile_options_input:
                    lines = tile_options_input.strip().split('\n')
                    tile_count = len([line for line in lines if line.strip()])
                    max_count = tile_count * 2
                
                min_discovery_count = st.slider(
                    "ìµœì†Œ ë°œê²¬ íšŸìˆ˜ í•„í„°", 1, max_count, 1, 1,
                    help=f"ì„¤ì •í•œ ê°’ ì´ìƒìœ¼ë¡œ ë°œê²¬ëœ ë°ì´í„°ë§Œ í‘œì‹œí•©ë‹ˆë‹¤. (ìµœëŒ€ {max_count}íšŒ)"
                )
                df_to_display = df_to_display[df_to_display['ë°œê²¬ íšŸìˆ˜'] >= min_discovery_count]

            filtered_df = filter_displayed_data(df_to_display, selected_patterns, min_id_length, show_all, require_digit)
            
            total_count = len(df_to_display)
            filtered_count = len(filtered_df)
            st.info(f"í•„í„°ë§ëœ í•­ëª©: {filtered_count} / {total_count} ê°œ")

            if not filtered_df.empty:
                df_with_selection = filtered_df.copy()
                df_with_selection.insert(0, "ì„ íƒ", True)
                st.session_state.data_with_selection = st.data_editor(
                    df_with_selection, 
                    key=f"{analysis_type}_editor",
                    hide_index=True
                )

        else:
            st.warning("ë¶„ì„ ê²°ê³¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        if st.session_state.visualization_images:
            with st.expander("ğŸ“Š íƒ€ì¼ ë¶„í•  ì‹œê°í™” ë³´ê¸°"):
                for idx, img in enumerate(st.session_state.visualization_images):
                    st.image(img, caption=f"í˜ì´ì§€ {idx + 1}", use_container_width=True)

        with st.expander("OCRë¡œ ì¶”ì¶œëœ ì „ì²´ í…ìŠ¤íŠ¸ ë³´ê¸° (ë””ë²„ê¹…ìš©)"):
            st.text_area("", st.session_state.raw_text, height=400)

    elif (analyze_button or reliability_button) and not uploaded_file:
        st.error("ë¶„ì„ì„ ì‹œì‘í•˜ê¸° ì „ì— PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    elif not analyze_button and not reliability_button:
        st.info("ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì„¤ì •ì„ ì¡°ì •í•œ ë’¤ ë¶„ì„ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")


with main_tab2:
    st.header("âœï¸ CSV í¸ì§‘ê¸°")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ë¶„ì„ ê²°ê³¼ì—ì„œ ì„ íƒí•œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
            if st.session_state.data_with_selection is not None and not st.session_state.data_with_selection.empty:
                selected_rows = st.session_state.data_with_selection[st.session_state.data_with_selection["ì„ íƒ"]]
                if not selected_rows.empty:
                    df_to_edit = selected_rows.drop(columns=["ì„ íƒ"])
                    
                    is_signal = predefined_patterns["ì‹ í˜¸ê¸° íŒ¨í„´ (ìˆ«ì ë˜ëŠ” ìˆ«ì/ìˆ«ì)"] in selected_patterns
                    is_balise = predefined_patterns["ë°œë¦¬ìŠ¤ íŒ¨í„´ (ë²”ìš©)"] in selected_patterns

                    if is_signal:
                        csv_list = []
                        for _, row in df_to_edit.iterrows():
                            loc, mid = row['location'], row['marker_id']
                            kpx = '0'
                            match = re.match(r"KM(\d+)\+(\d+)", loc)
                            if match:
                                try:
                                    kpx = int(match.group(1) + match.group(2)) * 100
                                except (ValueError, TypeError): pass
                            
                            parts = re.split(r'[-/]', mid)
                            for part in parts:
                                part = part.strip()
                                if part:
                                    csv_list.append({'KPPoint_X': kpx,'KPPoint_Y': '0', 'Direction': '0', 'Code': '2102', 'DefaultCode': '2102', 'DisplayName': part, 'PoleType': '2'})
                        st.session_state.edited_df = pd.DataFrame(csv_list, columns=['KPPoint_X', 'KPPoint_Y', 'Direction', 'Code', 'DefaultCode', 'DisplayName', 'PoleType'])
                    
                    elif is_balise:
                        csv_list = []
                        for _, row in df_to_edit.iterrows():
                            loc, mid = row['location'], row['marker_id']
                            kpx = '0'
                            match = re.match(r"KM(\d+)\+(\d+)", loc)
                            if match:
                                try:
                                    kpx = int(match.group(1) + match.group(2)) * 100
                                except (ValueError, TypeError): pass
                            csv_list.append({'KPPoint_X': kpx, 'KPPoint_Y': '0', 'Direction': '0', 'Name': mid})
                        st.session_state.edited_df = pd.DataFrame(csv_list, columns=['KPPoint_X', 'KPPoint_Y', 'Direction', 'Name'])

                    else:
                        st.session_state.edited_df = df_to_edit

                    st.success(f"{len(st.session_state.edited_df)}ê°œì˜ í–‰ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
                else:
                    st.warning("ë¶„ì„ ê²°ê³¼ íƒ­ì—ì„œ ê°€ì ¸ì˜¬ ë°ì´í„°ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            else:
                st.warning("ë¨¼ì € ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  ë°ì´í„°ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")

    with col2:
        uploaded_csv = st.file_uploader("ìƒˆ CSV íŒŒì¼ ì—…ë¡œë“œ", type=["csv"])
        if uploaded_csv is not None:
            try:
                df = pd.read_csv(uploaded_csv)
                st.session_state.edited_df = df
            except Exception as e:
                st.error(f"CSV íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                st.session_state.edited_df = None


    if st.session_state.edited_df is not None:
        st.info("í‘œë¥¼ ì§ì ‘ ìˆ˜ì •í•˜ê±°ë‚˜, í–‰ì„ ì¶”ê°€/ì‚­ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        st.session_state.edited_df = st.data_editor(st.session_state.edited_df, num_rows="dynamic", key="csv_editor")
        
        st.markdown("---")
        
        if not st.session_state.edited_df.empty:
            csv_data = st.session_state.edited_df.to_csv(index=False).encode('utf-8-sig')
            st.download_button(
                label="ìˆ˜ì •ëœ CSV ë‹¤ìš´ë¡œë“œ",
                data=csv_data,
                file_name="edited_data.csv",
                mime='text/csv'
            )

with main_tab3:
    st.header("ğŸ”§ XML ìƒì„±ê¸°")
    
    main_xml_file = st.file_uploader("1. ì›ë³¸ ì†ì„± XML íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=["xml"])
    signal_csv_file = st.file_uploader("2. ì‹ í˜¸ê¸° ë°ì´í„° CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”. (ì„ íƒ ì‚¬í•­)", type=["csv"])
    balis_csv_file = st.file_uploader("3. ë°œë¦¬ìŠ¤ ë°ì´í„° CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”. (ì„ íƒ ì‚¬í•­)", type=["csv"])
    
    output_filename = st.text_input("4. ì €ì¥í•  íŒŒì¼ ì´ë¦„", value="output.xml")

    if st.button("XML ìƒì„± ì‹œì‘", type="primary"):
        if main_xml_file:
            try:
                tree = ET.parse(main_xml_file)
                xml_root = tree.getroot()
                
                raillines_data, start_real_kp_value = extract_data_from_xml_root(xml_root)
                
                if raillines_data is not None:
                    # insertion_point = xml_root.find('.//Railway') or xml_root
                    # ìˆ˜ì •ëœ ì½”ë“œ
                    insertion_point = xml_root
                    
                    if signal_csv_file:
                        generate_signal_data(signal_csv_file, raillines_data, start_real_kp_value, insertion_point)
                    
                    if balis_csv_file:
                        generate_balis_data(balis_csv_file, raillines_data, start_real_kp_value, insertion_point)

                        
                    # rough_string = ET.tostring(xml_root, 'utf-8')
                    # reparsed = minidom.parseString(rough_string)
                    # pretty_xml_as_string = reparsed.toprettyxml(indent="  ", encoding="utf-8")
                    
                    # st.session_state.generated_xml = pretty_xml_as_string
                    
                    # minidomì„ ì‚¬ìš©í•œ pretty-printingì„ ì œê±°í•˜ì—¬ ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆì„ ì—†ì•±ë‹ˆë‹¤.
                    # encodingì„ 'unicode'ë¡œ ì„¤ì •í•˜ì—¬ ë¬¸ìì—´ë¡œ ì§ì ‘ ë³€í™˜í•œ í›„, ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•´ utf-8ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤.
                    xml_string = ET.tostring(xml_root, encoding='unicode')
                    
                    st.session_state.generated_xml = xml_string.encode('utf-8')
                    st.success(f"'{output_filename}' ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")

            except ET.ParseError as e:
                st.error(f"ì˜¤ë¥˜: ì›ë³¸ XML íŒŒì¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            except Exception as e:
                st.error(f"ì˜¤ë¥˜: XML ìƒì„± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        else:
            st.warning("ë¨¼ì € ì›ë³¸ ì†ì„± XML íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

    if st.session_state.generated_xml:
        st.download_button(
            label="ìƒì„±ëœ XML ë‹¤ìš´ë¡œë“œ",
            data=st.session_state.generated_xml,
            file_name=output_filename,
            mime="application/xml"
        )
